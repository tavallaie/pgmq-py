# src/pgmq/async_queue.py
"""
Asynchronous PGMQ client implementation.

This module provides the async PGMQueue class using asyncpg for high-performance
asyncio-based database operations.
"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Union
from datetime import datetime
import os
import logging
import warnings
import asyncpg
from asyncpg import Pool
import orjson  # Required for JSON serialization

from pgmq.base import BaseQueue, PGMQConfig
from pgmq import _sql
from pgmq.decorators import async_transaction
from pgmq.logger import log_with_context
from pgmq.messages import (
    Message,
    QueueMetrics,
    QueueRecord,
    TopicBinding,
    RoutingResult,
    BatchTopicResult,
    NotificationThrottle,
)


def _parse_jsonb(val) -> Any:
    """Parse asyncpg JSONB value."""
    if val is None:
        return None
    # asyncpg often returns JSONB as a string or bytes depending on schema
    if isinstance(val, (str, bytes)):
        return orjson.loads(val)
    # If it's already a dict/list, return as is
    return val


def _convert_sql(sql: str) -> str:
    """
    Convert psycopg style SQL (%s) to asyncpg style ($1, $2...).
    """
    count = sql.count("%s")
    if count == 0:
        return sql

    parts = sql.split("%s")
    result = []
    for i, part in enumerate(parts[:-1]):
        result.append(part)
        result.append(f"${i + 1}")
    result.append(parts[-1])
    return "".join(result)


@dataclass
class PGMQueue(BaseQueue):
    """
    Asynchronous PGMQueue client for PostgreSQL Message Queue operations.
    """

    # --- Backward Compatible Fields ---
    host: str = field(default_factory=lambda: os.getenv("PG_HOST", "localhost"))
    port: str = field(default_factory=lambda: os.getenv("PG_PORT", "5432"))
    database: str = field(default_factory=lambda: os.getenv("PG_DATABASE", "postgres"))
    username: str = field(default_factory=lambda: os.getenv("PG_USERNAME", "postgres"))
    password: str = field(default_factory=lambda: os.getenv("PG_PASSWORD", "postgres"))
    delay: int = 0
    vt: int = 30
    pool_size: int = 10
    verbose: bool = False
    log_filename: Optional[str] = None
    init_extension: bool = True
    structured_logging: bool = False
    log_rotation: bool = False
    log_rotation_size: str = "10 MB"
    log_retention: str = "1 week"

    # --- Internal Fields ---
    pool: Optional[Pool] = field(init=False, default=None)

    def __post_init__(self) -> None:
        """Initialize configuration after dataclass construction."""
        self.config = PGMQConfig(
            host=self.host,
            port=self.port,
            database=self.database,
            username=self.username,
            password=self.password,
            delay=self.delay,
            vt=self.vt,
            pool_size=self.pool_size,
            verbose=self.verbose,
            log_filename=self.log_filename,
            init_extension=self.init_extension,
            structured_logging=self.structured_logging,
            log_rotation=self.log_rotation,
            log_rotation_size=self.log_rotation_size,
            log_retention=self.log_retention,
        )
        super().__init__(config=self.config)

    async def init(self) -> None:
        """Initialize the asyncpg connection pool."""
        log_with_context(self.logger, logging.DEBUG, "Creating asyncpg pool")
        self.pool = await asyncpg.create_pool(
            self.config.async_dsn,
            min_size=1,
            max_size=self.config.pool_size,
        )

        if self.config.init_extension:
            async with self.pool.acquire() as conn:
                await conn.execute("CREATE EXTENSION IF NOT EXISTS pgmq CASCADE;")

    async def close(self) -> None:
        """Close the connection pool."""
        if self.pool:
            await self.pool.close()
            self.pool = None

    # =========================================================================
    # Connection Helpers
    # =========================================================================

    async def _execute(
        self, sql: str, params: Optional[tuple] = None, conn=None
    ) -> None:
        """Execute SQL without returning results."""
        sql = _convert_sql(sql)
        if conn:
            await conn.execute(sql, *params if params else ())
        else:
            async with self.pool.acquire() as c:
                await c.execute(sql, *params if params else ())

    async def _execute_with_result(
        self, sql: str, params: Optional[tuple] = None, conn=None
    ) -> List[tuple]:
        """Execute SQL and return all results."""
        sql = _convert_sql(sql)
        if conn:
            return await conn.fetch(sql, *params if params else ())
        else:
            async with self.pool.acquire() as c:
                return await c.fetch(sql, *params if params else ())

    async def _execute_one(
        self, sql: str, params: Optional[tuple] = None, conn=None
    ) -> Optional[tuple]:
        """Execute SQL and return first result."""
        results = await self._execute_with_result(sql, params, conn)
        return results[0] if results else None

    # =========================================================================
    # Queue Management
    # =========================================================================

    @async_transaction
    async def create_queue(self, queue: str, unlogged: bool = False, conn=None) -> None:
        """
        Create a new queue.

        Args:
            queue: Name of the queue.
            unlogged: If True, creates an unlogged table (faster writes, no crash safety).
            conn: Optional connection for transaction management. If provided, the operation
                  will be executed within this connection's transaction context.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Creating queue", queue=queue, unlogged=unlogged
        )
        sql = _sql.CREATE_UNLOGGED_QUEUE if unlogged else _sql.CREATE_QUEUE
        await self._execute(sql, (queue,), conn=conn)

    @async_transaction
    async def create_partitioned_queue(
        self,
        queue: str,
        partition_interval: Union[int, str] = 10000,
        retention_interval: Union[int, str] = 100000,
        conn=None,
    ) -> None:
        """
        Create a partitioned queue.

        Requires `pg_partman` extension to be installed in the database.

        Args:
            queue: Name of the queue.
            partition_interval: Interval for partitioning (e.g., 10000 rows or '1 day').
            retention_interval: Retention policy for partitions.
            conn: Optional connection for transaction management.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Creating partitioned queue", queue=queue
        )
        await self._execute(
            _sql.CREATE_PARTITIONED_QUEUE,
            (queue, str(partition_interval), str(retention_interval)),
            conn=conn,
        )

    @async_transaction
    async def drop_queue(self, queue: str, conn=None) -> bool:
        """
        Drop a queue and its associated table.

        Args:
            queue: Name of the queue.
            conn: Optional connection for transaction management.

        Returns:
            True if the queue was dropped successfully.
        """
        log_with_context(self.logger, logging.DEBUG, "Dropping queue", queue=queue)
        result = await self._execute_one(_sql.DROP_QUEUE, (queue,), conn=conn)
        return result[0] if result else False

    async def list_queues(self, conn=None) -> List[QueueRecord]:
        """
        List all queues with their metadata.

        Returns:
            List of QueueRecord objects containing queue metadata.
        """
        log_with_context(self.logger, logging.DEBUG, "Listing queues")
        warnings.warn(
            "list_queues() now returns List[QueueRecord] instead of List[str]. "
            "Access the queue name via the .queue_name attribute. "
            "This warning will be removed in a future version.",
            UserWarning,
            stacklevel=2,
        )
        rows = await self._execute_with_result(_sql.LIST_QUEUES, conn=conn)
        return [QueueRecord.from_row(row) for row in rows]

    async def validate_queue_name(self, queue_name: str, conn=None) -> bool:
        """
        Validate queue name format.

        Checks length and character restrictions.

        Args:
            queue_name: The queue name to validate.
            conn: Optional connection for execution.

        Returns:
            True if valid, False otherwise.
        """
        try:
            await self._execute(_sql.VALIDATE_QUEUE_NAME, (queue_name,), conn=conn)
            return True
        except Exception:
            return False

    # =========================================================================
    # Sending Messages
    # =========================================================================

    @async_transaction
    async def send(
        self,
        queue: str,
        message: Dict[str, Any],
        headers: Optional[Dict[str, Any]] = None,
        delay: Union[int, datetime, None] = None,
        tz: Union[int, datetime, None] = None,
        conn=None,
    ) -> int:
        """
        Send a single message to a queue.

        Args:
            queue: Name of the queue.
            message: Message payload (dictionary).
            headers: Optional dictionary of message headers.
            delay: Delay in seconds or timestamp before message becomes visible.
            tz: (Deprecated) Alias for delay.
            conn: Optional connection for transaction management.

        Returns:
            The message ID (bigint) of the sent message.
        """
        log_with_context(self.logger, logging.DEBUG, "Sending message", queue=queue)

        effective_delay = tz if tz is not None else delay

        has_headers = headers is not None
        has_delay = effective_delay is not None
        delay_is_ts = isinstance(effective_delay, datetime)

        sql = _sql.get_send_sql(has_headers, has_delay, delay_is_ts)

        msg_str = orjson.dumps(message).decode("utf-8")

        params: List[Any] = [queue, msg_str]
        if has_headers:
            headers_str = orjson.dumps(headers).decode("utf-8")
            params.append(headers_str)
        if has_delay:
            params.append(effective_delay)

        result = await self._execute_one(sql, tuple(params), conn=conn)
        return result[0] if result else -1

    @async_transaction
    async def send_batch(
        self,
        queue: str,
        messages: List[Dict[str, Any]],
        headers: Optional[List[Dict[str, Any]]] = None,
        delay: Union[int, datetime, None] = None,
        conn=None,
    ) -> List[int]:
        """
        Send multiple messages to a queue in a single operation.

        Args:
            queue: Name of the queue.
            messages: List of message payloads.
            headers: Optional list of headers (must match length of messages).
            delay: Optional delay for all messages.
            conn: Optional connection for transaction management.

        Returns:
            List of message IDs.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Sending batch",
            queue=queue,
            count=len(messages),
        )

        if not messages:
            return []

        if headers is not None and len(headers) != len(messages):
            raise ValueError("headers list must match messages list length")

        has_headers = headers is not None
        has_delay = delay is not None
        delay_is_ts = isinstance(delay, datetime)

        sql = _sql.get_send_batch_sql(has_headers, has_delay, delay_is_ts)

        msgs_str = [orjson.dumps(m).decode("utf-8") for m in messages]

        params: List[Any] = [queue, msgs_str]
        if has_headers:
            headers_str = [orjson.dumps(h).decode("utf-8") for h in headers]
            params.append(headers_str)
        if has_delay:
            params.append(delay)

        rows = await self._execute_with_result(sql, tuple(params), conn=conn)
        return [row[0] for row in rows]

    # =========================================================================
    # Topic-Based Routing
    # =========================================================================

    @async_transaction
    async def send_topic(
        self,
        routing_key: str,
        message: Dict[str, Any],
        headers: Optional[Dict[str, Any]] = None,
        delay: Optional[int] = None,
        conn=None,
    ) -> int:
        """
        Send a message to all queues bound to a topic routing key.

        Args:
            routing_key: The routing key (e.g., 'orders.created').
            message: Message payload.
            headers: Optional headers.
            delay: Optional delay.
            conn: Optional connection for transaction management.

        Returns:
            Number of queues the message was delivered to.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Sending topic message", routing_key=routing_key
        )

        has_headers = headers is not None
        has_delay = delay is not None

        sql = _sql.get_send_topic_sql(has_headers, has_delay)

        msg_str = orjson.dumps(message).decode("utf-8")
        params: List[Any] = [routing_key, msg_str]

        if has_headers:
            headers_str = orjson.dumps(headers).decode("utf-8")
            params.append(headers_str)
        if has_delay:
            params.append(delay)

        result = await self._execute_one(sql, tuple(params), conn=conn)
        return result[0] if result else 0

    @async_transaction
    async def send_batch_topic(
        self,
        routing_key: str,
        messages: List[Dict[str, Any]],
        headers: Optional[List[Dict[str, Any]]] = None,
        delay: Union[int, datetime, None] = None,
        conn=None,
    ) -> List[BatchTopicResult]:
        """
        Send a batch of messages to queues bound to a topic routing key.

        Args:
            routing_key: The routing key.
            messages: List of message payloads.
            headers: Optional list of headers.
            delay: Optional delay.
            conn: Optional connection for transaction management.

        Returns:
            List of BatchTopicResult mapping queue names to message IDs.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Sending batch topic",
            routing_key=routing_key,
            count=len(messages),
        )

        if not messages:
            return []

        has_headers = headers is not None
        has_delay = delay is not None
        delay_is_ts = isinstance(delay, datetime)

        sql = _sql.get_send_batch_topic_sql(has_headers, has_delay, delay_is_ts)

        msgs_str = [orjson.dumps(m).decode("utf-8") for m in messages]
        params: List[Any] = [routing_key, msgs_str]

        if has_headers:
            if len(headers) != len(messages):
                raise ValueError("headers list must match messages list length")
            headers_str = [orjson.dumps(h).decode("utf-8") for h in headers]
            params.append(headers_str)
        if has_delay:
            params.append(delay)

        rows = await self._execute_with_result(sql, tuple(params), conn=conn)
        return [BatchTopicResult.from_row(row) for row in rows]

    @async_transaction
    async def bind_topic(self, pattern: str, queue_name: str, conn=None) -> None:
        """
        Bind a queue to a topic pattern.

        Args:
            pattern: Topic pattern (e.g., 'orders.*').
            queue_name: Queue to bind.
            conn: Optional connection for transaction management.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Binding topic",
            pattern=pattern,
            queue=queue_name,
        )
        await self._execute(_sql.BIND_TOPIC, (pattern, queue_name), conn=conn)

    @async_transaction
    async def unbind_topic(self, pattern: str, queue_name: str, conn=None) -> bool:
        """
        Unbind a queue from a topic pattern.

        Args:
            pattern: Topic pattern.
            queue_name: Queue to unbind.
            conn: Optional connection for transaction management.

        Returns:
            True if successful.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Unbinding topic",
            pattern=pattern,
            queue=queue_name,
        )
        result = await self._execute_one(
            _sql.UNBIND_TOPIC, (pattern, queue_name), conn=conn
        )
        return result[0] if result else False

    async def list_topic_bindings(
        self, queue_name: Optional[str] = None, conn=None
    ) -> List[TopicBinding]:
        """
        List all topic bindings.

        Args:
            queue_name: Optional filter by queue name.
            conn: Optional connection.

        Returns:
            List of TopicBinding objects.
        """
        if queue_name:
            rows = await self._execute_with_result(
                _sql.LIST_TOPIC_BINDINGS_FOR_QUEUE, (queue_name,), conn=conn
            )
        else:
            rows = await self._execute_with_result(_sql.LIST_TOPIC_BINDINGS, conn=conn)
        return [TopicBinding.from_row(row) for row in rows]

    async def test_routing(self, routing_key: str, conn=None) -> List[RoutingResult]:
        """
        Test which queues match a routing key without sending a message.

        Args:
            routing_key: The routing key to test.
            conn: Optional connection.

        Returns:
            List of RoutingResult objects.
        """
        rows = await self._execute_with_result(
            _sql.TEST_ROUTING, (routing_key,), conn=conn
        )
        return [RoutingResult.from_row(row) for row in rows]

    # =========================================================================
    # Reading Messages
    # =========================================================================

    @async_transaction
    async def read(
        self,
        queue: str,
        vt: Optional[int] = None,
        qty: int = 1,
        conditional: Optional[Dict[str, Any]] = None,
        conn=None,
    ) -> Optional[Union[Message, List[Message]]]:
        """
        Read messages from a queue.

        Args:
            queue: Name of the queue.
            vt: Visibility timeout in seconds.
            qty: Maximum number of messages to read.
            conditional: Optional JSONB condition for filtering.
            conn: Optional connection for transaction management.

        Returns:
            A single Message or list of Messages, or None if empty.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Reading messages", queue=queue, qty=qty
        )

        actual_vt = vt or self.vt

        if conditional:
            sql = _sql.READ_CONDITIONAL
            cond_str = orjson.dumps(conditional).decode("utf-8")
            params = (queue, actual_vt, qty, cond_str)
        else:
            sql = _sql.READ
            params = (queue, actual_vt, qty)

        rows = await self._execute_with_result(sql, params, conn=conn)
        messages = [Message.from_row(row, _parse_jsonb) for row in rows]

        if qty == 1:
            return messages[0] if messages else None
        return messages

    async def read_batch(
        self, queue: str, vt: Optional[int] = None, batch_size: int = 1, conn=None
    ) -> List[Message]:
        """
        Read a batch of messages (backward compatibility alias).

        Args:
            queue: Name of the queue.
            vt: Visibility timeout.
            batch_size: Number of messages.
            conn: Optional connection.

        Returns:
            List of Messages.
        """
        result = await self.read(queue, vt=vt, qty=batch_size, conn=conn)
        if result is None:
            return []
        if isinstance(result, list):
            return result
        return [result]

    @async_transaction
    async def read_with_poll(
        self,
        queue: str,
        vt: Optional[int] = None,
        qty: int = 1,
        max_poll_seconds: int = 5,
        poll_interval_ms: int = 100,
        conditional: Optional[Dict[str, Any]] = None,
        conn=None,
    ) -> List[Message]:
        """
        Read messages with long-polling.

        Args:
            queue: Name of the queue.
            vt: Visibility timeout.
            qty: Max messages.
            max_poll_seconds: Max seconds to wait for a message.
            poll_interval_ms: Milliseconds between polls.
            conditional: Optional filter condition.
            conn: Optional connection for transaction management.

        Returns:
            List of Messages.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Reading with poll", queue=queue, qty=qty
        )

        actual_vt = vt or self.vt

        if conditional:
            sql = _sql.READ_WITH_POLL_CONDITIONAL
            cond_str = orjson.dumps(conditional).decode("utf-8")
            params = (
                queue,
                actual_vt,
                qty,
                max_poll_seconds,
                poll_interval_ms,
                cond_str,
            )
        else:
            sql = _sql.READ_WITH_POLL
            params = (queue, actual_vt, qty, max_poll_seconds, poll_interval_ms)

        rows = await self._execute_with_result(sql, params, conn=conn)
        return [Message.from_row(row, _parse_jsonb) for row in rows]

    # =========================================================================
    # FIFO Operations
    # =========================================================================

    @async_transaction
    async def read_grouped(
        self, queue: str, vt: Optional[int] = None, qty: int = 1, conn=None
    ) -> List[Message]:
        """
        FIFO grouped read (SQS-style batch filling).

        Args:
            queue: Name of the queue.
            vt: Visibility timeout.
            qty: Max messages.
            conn: Optional connection for transaction management.

        Returns:
            List of Messages.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Reading grouped", queue=queue, qty=qty
        )
        params = (queue, vt or self.vt, qty)
        rows = await self._execute_with_result(_sql.READ_GROUPED, params, conn=conn)
        return [Message.from_row(row, _parse_jsonb) for row in rows]

    @async_transaction
    async def read_grouped_with_poll(
        self,
        queue: str,
        vt: Optional[int] = None,
        qty: int = 1,
        max_poll_seconds: int = 5,
        poll_interval_ms: int = 100,
        conn=None,
    ) -> List[Message]:
        """FIFO grouped read with long-polling."""
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Reading grouped with poll",
            queue=queue,
            qty=qty,
        )
        params = (queue, vt or self.vt, qty, max_poll_seconds, poll_interval_ms)
        rows = await self._execute_with_result(
            _sql.READ_GROUPED_WITH_POLL, params, conn=conn
        )
        return [Message.from_row(row, _parse_jsonb) for row in rows]

    @async_transaction
    async def read_grouped_rr(
        self, queue: str, vt: Optional[int] = None, qty: int = 1, conn=None
    ) -> List[Message]:
        """FIFO round-robin read."""
        log_with_context(
            self.logger, logging.DEBUG, "Reading grouped RR", queue=queue, qty=qty
        )
        params = (queue, vt or self.vt, qty)
        rows = await self._execute_with_result(_sql.READ_GROUPED_RR, params, conn=conn)
        return [Message.from_row(row, _parse_jsonb) for row in rows]

    @async_transaction
    async def read_grouped_rr_with_poll(
        self,
        queue: str,
        vt: Optional[int] = None,
        qty: int = 1,
        max_poll_seconds: int = 5,
        poll_interval_ms: int = 100,
        conn=None,
    ) -> List[Message]:
        """FIFO round-robin read with poll."""
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Reading grouped RR with poll",
            queue=queue,
            qty=qty,
        )
        params = (queue, vt or self.vt, qty, max_poll_seconds, poll_interval_ms)
        rows = await self._execute_with_result(
            _sql.READ_GROUPED_RR_WITH_POLL, params, conn=conn
        )
        return [Message.from_row(row, _parse_jsonb) for row in rows]

    # =========================================================================
    # Pop
    # =========================================================================

    @async_transaction
    async def pop(
        self, queue: str, qty: int = 1, conn=None
    ) -> Optional[Union[Message, List[Message]]]:
        """
        Pop messages (read and delete immediately).

        Args:
            queue: Name of the queue.
            qty: Number of messages.
            conn: Optional connection for transaction management.

        Returns:
            Message or list of Messages.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Popping messages", queue=queue, qty=qty
        )
        rows = await self._execute_with_result(_sql.POP, (queue, qty), conn=conn)
        messages = [Message.from_row(row, _parse_jsonb) for row in rows]

        if qty == 1:
            return messages[0] if messages else None
        return messages

    # =========================================================================
    # Deleting and Archiving
    # =========================================================================

    @async_transaction
    async def delete(self, queue: str, msg_id: int, conn=None) -> bool:
        """
        Delete a single message.

        Args:
            queue: Name of the queue.
            msg_id: ID of the message.
            conn: Optional connection for transaction management.

        Returns:
            True if successful.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Deleting message", queue=queue, msg_id=msg_id
        )
        result = await self._execute_one(_sql.DELETE, (queue, msg_id), conn=conn)
        return result[0] if result else False

    @async_transaction
    async def delete_batch(
        self, queue: str, msg_ids: List[int], conn=None
    ) -> List[int]:
        """
        Delete multiple messages.

        Args:
            queue: Name of the queue.
            msg_ids: List of message IDs.
            conn: Optional connection for transaction management.

        Returns:
            List of deleted message IDs.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Deleting batch",
            queue=queue,
            count=len(msg_ids),
        )
        rows = await self._execute_with_result(
            _sql.DELETE_BATCH, (queue, msg_ids), conn=conn
        )
        return [row[0] for row in rows]

    @async_transaction
    async def archive(self, queue: str, msg_id: int, conn=None) -> bool:
        """
        Archive a single message.

        Args:
            queue: Name of the queue.
            msg_id: ID of the message.
            conn: Optional connection for transaction management.

        Returns:
            True if successful.
        """
        log_with_context(
            self.logger, logging.DEBUG, "Archiving message", queue=queue, msg_id=msg_id
        )
        result = await self._execute_one(_sql.ARCHIVE, (queue, msg_id), conn=conn)
        return result[0] if result else False

    @async_transaction
    async def archive_batch(
        self, queue: str, msg_ids: List[int], conn=None
    ) -> List[int]:
        """
        Archive multiple messages.

        Args:
            queue: Name of the queue.
            msg_ids: List of message IDs.
            conn: Optional connection for transaction management.

        Returns:
            List of archived message IDs.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Archiving batch",
            queue=queue,
            count=len(msg_ids),
        )
        rows = await self._execute_with_result(
            _sql.ARCHIVE_BATCH, (queue, msg_ids), conn=conn
        )
        return [row[0] for row in rows]

    @async_transaction
    async def purge(self, queue: str, conn=None) -> int:
        """
        Purge all messages from a queue.

        Args:
            queue: Name of the queue.
            conn: Optional connection for transaction management.

        Returns:
            Number of messages purged.
        """
        log_with_context(self.logger, logging.DEBUG, "Purging queue", queue=queue)
        result = await self._execute_one(_sql.PURGE_QUEUE, (queue,), conn=conn)
        return result[0] if result else 0

    # =========================================================================
    # Visibility Timeout
    # =========================================================================

    @async_transaction
    async def set_vt(
        self,
        queue: str,
        msg_id: Union[int, List[int]],
        vt: Union[int, datetime],
        conn=None,
    ) -> Optional[Union[Message, List[Message]]]:
        """
        Set visibility timeout for message(s).

        Args:
            queue: Name of the queue.
            msg_id: Message ID or list of IDs.
            vt: Visibility timeout (seconds or timestamp).
            conn: Optional connection for transaction management.

        Returns:
            Updated Message or list of Messages.
        """
        is_batch = isinstance(msg_id, list)
        vt_is_timestamp = isinstance(vt, datetime)

        log_with_context(
            self.logger, logging.DEBUG, "Setting VT", queue=queue, is_batch=is_batch
        )

        sql = _sql.get_set_vt_sql(is_batch, vt_is_timestamp)

        params = (queue, msg_id, vt)

        rows = await self._execute_with_result(sql, params, conn=conn)
        messages = [Message.from_row(row, _parse_jsonb) for row in rows]

        if is_batch:
            return messages
        return messages[0] if messages else None

    # =========================================================================
    # Metrics
    # =========================================================================

    async def metrics(self, queue: str, conn=None) -> QueueMetrics:
        """
        Get metrics for a specific queue.

        Args:
            queue: Name of the queue.
            conn: Optional connection.

        Returns:
            QueueMetrics object.
        """
        log_with_context(self.logger, logging.DEBUG, "Getting metrics", queue=queue)
        row = await self._execute_one(_sql.METRICS, (queue,), conn=conn)
        if not row:
            raise ValueError(f"Queue '{queue}' not found")
        return QueueMetrics.from_row(row)

    async def metrics_all(self, conn=None) -> List[QueueMetrics]:
        """
        Get metrics for all queues.

        Args:
            conn: Optional connection.

        Returns:
            List of QueueMetrics.
        """
        log_with_context(self.logger, logging.DEBUG, "Getting all metrics")
        rows = await self._execute_with_result(_sql.METRICS_ALL, conn=conn)
        return [QueueMetrics.from_row(row) for row in rows]

    # =========================================================================
    # Notifications
    # =========================================================================

    @async_transaction
    async def enable_notify(
        self, queue: str, throttle_interval_ms: int = 250, conn=None
    ) -> None:
        """
        Enable NOTIFY for queue insertions.

        Args:
            queue: Name of the queue.
            throttle_interval_ms: Minimum interval between notifications.
            conn: Optional connection for transaction management.
        """
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Enabling notifications",
            queue=queue,
            throttle=throttle_interval_ms,
        )
        await self._execute(
            _sql.ENABLE_NOTIFY, (queue, throttle_interval_ms), conn=conn
        )

    @async_transaction
    async def disable_notify(self, queue: str, conn=None) -> None:
        """Disable NOTIFY for a queue."""
        log_with_context(
            self.logger, logging.DEBUG, "Disabling notifications", queue=queue
        )
        await self._execute(_sql.DISABLE_NOTIFY, (queue,), conn=conn)

    @async_transaction
    async def update_notify(
        self, queue: str, throttle_interval_ms: int, conn=None
    ) -> None:
        """Update notification throttle interval."""
        log_with_context(
            self.logger,
            logging.DEBUG,
            "Updating notification throttle",
            queue=queue,
            throttle=throttle_interval_ms,
        )
        await self._execute(
            _sql.UPDATE_NOTIFY, (queue, throttle_interval_ms), conn=conn
        )

    async def list_notify_throttles(self, conn=None) -> List[NotificationThrottle]:
        """List all notification configurations."""
        rows = await self._execute_with_result(_sql.LIST_NOTIFY_THROTTLES, conn=conn)
        return [NotificationThrottle.from_row(row) for row in rows]

    # =========================================================================
    # Utilities
    # =========================================================================

    async def validate_routing_key(self, routing_key: str, conn=None) -> bool:
        """Validate routing key format."""
        try:
            await self._execute(_sql.VALIDATE_ROUTING_KEY, (routing_key,), conn=conn)
            return True
        except Exception:
            return False

    async def validate_topic_pattern(self, pattern: str, conn=None) -> bool:
        """Validate topic pattern format."""
        try:
            await self._execute(_sql.VALIDATE_TOPIC_PATTERN, (pattern,), conn=conn)
            return True
        except Exception:
            return False

    @async_transaction
    async def create_fifo_index(self, queue: str, conn=None) -> None:
        """Create FIFO index."""
        log_with_context(self.logger, logging.DEBUG, "Creating FIFO index", queue=queue)
        await self._execute(_sql.CREATE_FIFO_INDEX, (queue,), conn=conn)

    async def create_fifo_indexes_all(self, conn=None) -> None:
        """Create FIFO indexes on all queues."""
        log_with_context(self.logger, logging.DEBUG, "Creating all FIFO indexes")
        await self._execute(_sql.CREATE_FIFO_INDEXES_ALL, conn=conn)

    @async_transaction
    async def convert_archive_partitioned(
        self,
        queue: str,
        partition_interval: Union[int, str] = 10000,
        retention_interval: Union[int, str] = 100000,
        leading_partition: int = 10,
        conn=None,
    ) -> None:
        """Convert archive to partitioned."""
        log_with_context(
            self.logger, logging.DEBUG, "Converting archive to partitioned", queue=queue
        )
        await self._execute(
            _sql.CONVERT_ARCHIVE_PARTITIONED,
            (
                queue,
                str(partition_interval),
                str(retention_interval),
                leading_partition,
            ),
            conn=conn,
        )

    @async_transaction
    async def detach_archive(self, queue: str, conn=None) -> None:
        """Detach archive (deprecated)."""
        log_with_context(
            self.logger, logging.DEBUG, "Detaching archive (deprecated)", queue=queue
        )
        await self._execute(_sql.DETACH_ARCHIVE, (queue,), conn=conn)
